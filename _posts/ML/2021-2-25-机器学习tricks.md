---
title: tricks
tags:
  - ML
---

总则：**data > model > feature engineering > ensemble**

# Bias

​		在训练数据上面，我们可以进行交叉验证（Cross-Validation）。

​		 K-fold Cross Validation（K 折交叉验证）：初始采样分割成 K 个子样本，一个单独的子样本被保留作为验证模型的数据，其他 K-1 个样本用来训练。交叉验证重复 K 次，每个子样本验证一次，平均 K 次的结果或者使用其它结合方式，最终得到一个单一估测。

> 当 K 值大的时候， 我们会有更少的 Bias（偏差）、更多的 Variance（方差）。
> 当 K 值小的时候， 我们会有更多的 Bias（偏差）、更少的 Variance。

​		Bias和Variance是针对**Generalization**（一般化，泛化）来说的。

​		在机器学习中，我们用训练数据集去训练（学习）一个model（模型），通常的做法是定义一个Loss function（误差函数），通过将这个Loss（或者叫error）的最小化过程，来提高模型的性能（performance）。然而我们学习一个模型的目的是为了解决实际的问题（或者说是训练数据集这个领域（field）中的一般化问题），单纯地将训练数据集的loss最小化，并不能保证在解决更一般的问题时模型仍然是最优，甚至不能保证模型是可用的。这个训练数据集的loss与一般化的数据集的loss之间的差异就叫做generalization error。

​		**generalization error又细分为Bias和Variance两个部分。**

> Bias是 “用**所有可能的**训练数据集训练出的**所有模型**的输出的**平均值**” 与 “真实模型”的输出值之间的差异；
> Variance则是“**不同的训练数据集**训练出的模型”的输出值之间的差异。

# tricks

虽然机器模型变得越来越复杂,不能像过去那样对LR模型那样直接修改权重去拟合test的bias,但是对ai这个黑盒子,我们还是可以:

1.有选择的控制黑盒子的输入,根据test bias的分布,我们想得到什么,我们就输入什么.毕竟现在的模型记忆力很强.

2.有选择的修改黑盒子的输出,根据test bias的分布,我们想得到什么,我们就修改什么.然后封装起来好像就是模型输出的一样.

## **Less is More**

**[Santander Product Recommendation](https://link.zhihu.com/?target=https%3A//www.kaggle.com/c/santander-product-recommendation/)**

这个数据集提供银行用户2015年1月到2016年5月的产品购买数据,要求预测用户2016年6月购买什么产品.按照通常的观点,训练数据都是多多益善的. 但是[BreakfastPirate](https://link.zhihu.com/?target=https%3A//www.kaggle.com/breakfastpirate) 在 [When Less is More](https://link.zhihu.com/?target=https%3A//www.kaggle.com/c/santander-product-recommendation/discussion/25579) 里提出,因为每年6月份是银行的结算日,此时的用户行为和平时不一样的.只使用2015年6月一个月的数据,效果要比使用17个月的数据效果要好.

这个例子说明当train 和test 存在bias的时候 ,只使用train里那部分和test相似的数据可能效果更好.

## **warm up**

**[Freesound Audio Tagging 2019](https://link.zhihu.com/?target=https%3A//www.kaggle.com/c/freesound-audio-tagging-2019)**

训练集是由4970个人工标注的curated音频片段和19815个机器标注的noisy音频片段组成,而test和人工标注的的训练集同源,但是如果只用4970个数据显然太少了,但深度学习的好处就是可以迁移学习,所以可以用noisy数据做warm up训练,最后再用curated数据在warm up后的预训练模型上继续训练.在lwlrap这个metric可以比只用curated训练提升一个百分点.

这个例子说明深度学习通过warm up,迁移学习可以一定程度利用那些和test存在bias的数据

树模型也有类似利用存在bias数据的技巧,见**[嫁接学习](https://zhuanlan.zhihu.com/p/98728768)**

## **pseudo label**

**[Toxic Comment Classification Challenge](https://link.zhihu.com/?target=https%3A//www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)**

这个数据由于发生了test泄露,因此中途增加了后续标注的新test数据.由于数据的数据源和标注的人不一样,所以旧的train数据和新的test存在bias.一个解决方法就是用train训练的模型给新test打标签,然后把置信度高的test加入train里训练.AUC从0.9880 提升到了 0.9885.

无论数据是否有bias,一般pseudo label都能涨分,但在训练集和测试机分布不完全一致的时候，将test通pseudo label过加入训练,效果会更加显著。

## **magic number**

**[Sberbank Russian Housing Market](https://link.zhihu.com/?target=https%3A//www.kaggle.com/c/sberbank-russian-housing-market)**

这个题用2011年8月到2015年6月的莫斯科房价数据来预测 2015年7月到2016年5月的房价,而由于众所周知的原因,货币卢布在那几年发生了通货膨胀,导致test的房价数据要高于train的数据,因此所有的方案都在模型结果后处理乘了一个放大系数,在test上可以带来显著的提升,大家称之为magic number. 虽然这种人工后处理的方式很不机器学习.但这就是real world. 最后出题方 Sberbank 的 Data Science Lead 也在[Question to admins about rules](https://link.zhihu.com/?target=https%3A//www.kaggle.com/c/sberbank-russian-housing-market/discussion/35215) 认可了这种后处理方法.

事实上,后处理也在大量的由时间导致的bias类数据被使用.除了乘系数,还有在分类里调阈值等等

关键词: **outlier**

因为机器学习不能完全解决bias,所以上一段提到乘系数,调阈值,或者人工规则在内行人里是被认可的,但如果遇到外行评审,他会认为你不是用机器学习,从而否定你的方案.这时候可能就要包装自己方案,比如直接乘一个放大系数不行,那我们就可以把train里面一些label偏低的样本作为outlier舍弃掉或降低weight,一样能起到放大模型输出的效果,这样就能忽悠评审了. 其他规则方法也可以用类似调整train样本方法来把规则手法掩盖起来.

# **[机器学习工作内容和kaggle竞赛有什么区别？](https://www.zhihu.com/question/295475618)** 

实际工作中，在model没有颠覆性的突破的情况下，比起调参和特征工程，可能去搜集更多样本和更多表收益更大。

实际在做的项目中, 所用到的数据全都是自己去工厂车间采集的, 还有一部分数据是通过各种方法去跟厂家要的, 然后得自己定好数据制作的标准, 比如数据要覆盖到多少类别,怎么保证数据多样性和代表性等等,然后设计模型, 设计metrics,针对模型预测表现去调整数据的采样规则. 最后还得把模型部署到端设备上, 这时候就得做模型的转换了, 这又是一个大坑,如果在模型设计的时候用了很多炫酷的tricks这个时候就会给你带来麻烦, 很多较新的层经常是无法通过转换的, 不然就是转换成功, 发现tensorrt这类工具并不支持这个层. 所以,这一部分的工作会占用绝大多数的时间. 这也就是kaggle和实际工作的一些区别了.