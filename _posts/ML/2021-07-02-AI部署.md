---
title: AI部署
tags: ML
---



# AI部署

## AI部署的基本步骤或要求

- 训练一个模型或者拿一个预训练好的模型
- 针对不同平台对生成的模型进行转换，也就是俗称的parse、convert，即前端解释器
- 针对转化后的模型进行优化
- 在特定的平台(嵌入端或者服务端)成功运行优化后的模型
- 在模型可以运行的基础上，保证模型的速度、精度和稳定性

对于硬件公司来说，需要将深度学习算法部署到**性能低到离谱**的开发板上。在算法层面优化模型是一方面，但更重要的是从底层优化这个模型，涉及到部署落地方面的各个知识(手写汇编算子加速、算子融合等等)；

对于软件公司来说，需要将算法运行到服务器上，服务器可以是布满2080TI的高性能GPU机器，QPS请求足够高的话，需要的服务器数量也是相当之大的。

算法部署算是开发，不仅需要和训练好的模型打交道，有时候也会干一些粗活累活(也就是dirty work)，用C++、cuda写算子(预处理、op、后处理等等)去实现一些独特的算子。也需要经常调bug、联合编译、动态静态库混搭等等。

## 部署

部署不光是**从研究环境到生产环境**的转换，更多的是模型速度的提升和稳定性的提升。

- 模型结构
- 剪枝
- 蒸馏
- 稀疏化训练
- 量化训练
- 算子融合、计算图优化、底层优化

## 部署框架

Tensoflow lite

Pytorch C++

ONNX

NCNN

MNN

TNN

TensorRT

## 案例

模型是使用**Pytorch**训练的，部署的平台是英伟达的GPU服务器。

训练好的模型通过以下几种方式转换：

- Pytorch->ONNX->trt **onnx2trt** 最成熟
- Pytorch->trt **torch2trt** 较灵活
- Pytorch->torchscipt->trt **trtorch** 

常见的服务部署搭配：
- triton server + TensorRT/libtorch
- flask + Pytorch
- Tensorflow Server

## ONNX

ONNX结构的定义基本都在这一个[onnx.proto](https://github.com/onnx/onnx/blob/master/onnx/onnx.proto)文件里面

- ModelProto
- GraphProto
- NodeProto
- AttributeProto
- ValueInfoProto
- TensorProto

将ONNX模型load进来之后，得到的是一个`ModelProto`，包含了一些版本信息，生产者信息和一个非常重要的`GraphProto`；

在`GraphProto`中包含了四个关键的repeated数组，分别是`node`(`NodeProto`类型)，`input`(`ValueInfoProto`类型)，`output`(`ValueInfoProto`类型)和`initializer`(`TensorProto`类型)，其中`node`中存放着模型中的所有计算节点，不仅仅包含我们一般理解中的图片输入的那个节点，还包含了模型当中所有权重。`input`中存放着模型所有的输入节点，`output`存放着模型所有的输出节点，`initializer`存放着模型所有的权重；

节点与节点之间的拓扑是如何定义的呢？每个计算节点都同样会有`input`和`output`这样的两个数组(不过都是普通的string类型)，通过`input`和`output`的指向关系快速构建出一个深度学习模型的拓扑图。

最后每个计算节点当中还包含了一个`AttributeProto`数组，用于描述该节点的属性，例如`Conv`层的属性包含`group`，`pads`和`strides`等等，具体每个计算节点的属性、输入和输出可以参考这个[Operators.md](https://github.com/onnx/onnx/blob/master/docs/Operators.md)文档。



### onnx的部署

直接使用ONNX模型来做部署的话，有下列几种情况：第一种情况，目标平台是CUDA或者X86的话，又怕环境配置麻烦采坑，比较推荐使用的是微软的[onnxruntime](https://microsoft.github.io/onnxruntime/)；第二种情况，而如果目标平台是CUDA又追求极致的效率的话，可以考虑转换成TensorRT；第三种情况，如果目标平台是ARM或者其他IoT设备，考虑使用端侧推理框架了，例如NCNN、MNN和MACE等等。
