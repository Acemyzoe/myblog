---
title: 模型工程化部署
tags:
  - ML
typora-copy-images-to: ./
---

## 深度学习框架

一般流程：模型设计和训练，针对推断框架的模型转换，模型部署。

一般从离线训练到在线部署，训练需要依赖离线训练框架（tensorflow、pytorch等），部署需要依赖在线推理的框架（如TFlite、阿里的MNN、腾讯的NCNN等）。对于NVIDIA的产品，一般都会使用TensorRT来加速推理，TensorRT用了CUDA、CUDNN，而且还有图优化、fp16、int8量化等。

> **TensorFlow Lite**
>
> 2017年5月17日 Goole在I/O大会推出TensorFlow Lite，是专门为移动设备而优化的 TensorFlow 版本。
>
> TensorFlow Lite 具备以下三个重要功能：
>
> 轻量级（Lightweight）：支持机器学习模型的推理在较小二进制数下进行，能快速初始化/启动
>
> 跨平台（Cross-platform）：可以在许多不同的平台上运行，现在支持 Android 和 iOS
>
> 快速（Fast）：针对移动设备进行了优化，包括大大减少了模型加载时间、支持硬件加速
>
> 模块如下:
>
> TensorFlow Model: 存储在硬盘上已经训练好的 TensorFlow 模型
>
> TensorFlow Lite Converter: 将模型转换为 TensorFlow Lite 文件格式的程序
>
> TensorFlow Lite Model File: 基于 FlatBuffers 的模型文件格式，针对速度和大小进行了优化。
>
> TensorFlow Lite 目前支持很多针对移动端训练和优化好的模型。

## 部署环境及方式

### 部署环境

包括硬件和软件，首先需要明确的是模型需要部署在什么样的硬件环境中：服务器（公有云或者私有云，能否支持GPU显卡加速），嵌入式，或者移动端；硬件决定了上面可以跑的软件系统，也就在很大程度上决定了需要使用的技术栈（框架、语言等）。

### 部署方式

取决于模型推理计算的环境是在服务器还是终端。负荷预测部分采用Online方式，异常识别部分采用Offline方式。

**Online 方式**

首先在嵌入式终端做初步数据预处理，然后把数据传到服务器进行预测后存储结果或将结果返回终端。

优点：部署相对简单，现成的框架（tensorflow，pytorch等) 做下封装就可以直接拿来用；使用服务器进行计算，性能强，能够处理比较大的模型。

缺点：不适合实时性要求高的应用。

**Offline 方式**

根据硬件的性能选择模型，离线训练得到模型，在嵌入式终端上进行推理。

优点：不需要使用网络，可以保护隐私

缺点：计算的性能、耗时等取决于终端的性能，有些模型只能使用CPU，精度可能会有影响，无法进行类似云端的大规模分布式训练；终端部署相对较麻烦，需要针对终端进行优化；大模型耗费大量的资源（计算、内存、存储、电）。

## 部署类型

主要分实时在线预测和离线批量预测两大类。

针对实时预测，部署服务通常提供API以供客户端调用，比如使用流行的REST接口；

离线预测一般是在服务器端部署，比如设定定时任务，每天定时从数据库中读取某一时间段数据，并且把预测结果再写入数据库以供后续的使用。

按照需求，该项目采用离线预测即可。

## 部署方法

首先模型最好用c++重写，像tensorflow可以直接编译成二进制。c++对矩阵、张量、图像运算、并行计算可以使用opencv，openml，opencl，opengl，cuda、cudnn等加速，性能要求不高可以直接使用TFlite等框架。放入嵌入式移动设备时，需要有专门的加速芯片，对权重进行剪枝，可能要重新编译。

现在模型部署主流的做法是将模型和模型的服务环境做成docker image。好处是屏蔽了模型对环境的依赖，因为深度学习模型在服务的时候可能对各种框架版本和依赖库有要求。将模型通过docker服务化后意味着深度学习模型可以在各种环境使用，比如云端直接通过k8s调度拉起，采用web service，开放API 接口，即简单的网页服务开发。或者在一些IOT领域，比方说一些嵌入式终端也可以通过拉起镜像服务的方式使用模型。

例如，使用TensorFlow框架，导出pd格式的模型，在嵌入式终端中使用docker拉取实例，直接用tensorflow的core lib加载模型。在服务器上，用Docker做容器，用Kubernetes做集群，用python的flask做成web微服务。

![模型部署](/home/ace/Desktop/github/acemyzoe.github.io/_posts/ML/模型部署.jpg)

## 性能要求

API的各种性能指标，包括服务的稳定性等。

针对实时预测主要性能指标包括单条响应时间（RT）和吞吐量（Throughput）等。

对于离线批量预测来说主要在于吞吐量，确保在规定的时间完成批量预测任务。

基于现有指标，即系统对延迟（比如100ms以内）和吞吐（比如数万的qps），使用docker即可。

## 模型管理

机器学习项目始终是一个循环迭代的过程，需要能够快速上线新的模型，并且在不影响调用端的情况下自由切换模型版本、进行AB测试等。