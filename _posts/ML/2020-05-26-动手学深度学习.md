---
title: 《动手学深度学习》
tags:
  - ML
---



# [《动手学深度学习》](https://zh.d2l.ai)

## 起源

`赫布理论`支撑今日深度学习的随机梯度下降算法的基石：强化合意的行为、惩罚不合意的行为，最终获得优良的神经网络参数。

`神经网络核心原则`：

- 交替使用线性处理单元与非线性处理单元，它们经常被称为“层”。
- 使用链式法则（即反向传播）来更新网络的参数。

## 发展

数据量和计算力的发展导致了机器学习和统计学的最优选择从广义线性模型及核方法变化为深度多层神经网络。

- 优秀的容量控制方法，如丢弃法使大型网络的训练不再受制于过拟合，加入噪声（训练时随机将权重替换为随机的数字）。

- 注意力机制：如何在不增加参数的情况下扩展一个系统的记忆容量和复杂度。

  > 与其在像机器翻译这样的任务中记忆整个句子，不如记忆指向翻译的中间状态的指针。

- 记忆网络

- 生成对抗网络

  > 传统上，用在概率分布估计和生成模型上的统计方法更多地关注于找寻正确的概率分布，以及正确的采样算法。生成对抗网络的关键创新在于将采样部分替换成了任意的含有可微分参数的算法。

- 构建分布式并行训练算法的能力：随机梯度下降需要相对更小的批量。与此同时，更小的批量也会降低GPU的效率。

## 特点

机器学习研究如何使计算机系统利用经验改善性能。它是人工智能领域的分支，也是实现人工智能的一种手段。

深度学习是具有多级表示的表征学习方法。表征学习关注如何自动找出表示数据的合适方式，以便更好地将输入变换为正确的输出。

深度学习的一个外在特点是端到端的训练，并不是将单独调试的部分拼凑起来组成一个系统，而是将整个系统组建好之后一起训练。

# 《30天吃掉tensorflow》